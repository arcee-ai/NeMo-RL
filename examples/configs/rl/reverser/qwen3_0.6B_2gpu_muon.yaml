# GRPO RL Configuration (New Format)
rollouts:
  group_size: 16
  max_concurrent_rollouts: null
  use_leave_one_out_baseline: true
  use_std_normalization: true
  max_staleness: 1

policy:
  model_name: "PrimeIntellect/Qwen3-0.6B-Reverse-Text-SFT"
  max_total_sequence_length: 512

  training:
    global_num_bins: 256
    micro_batch_size: 4
    dtype: "bfloat16"
    activation_checkpointing: false
    max_grad_norm: 1.0

    parallelism:
      tp_size: 1
      ep_size: 1
      dp_replicate: 1

    optimizer:
      name: "dion.Muon"
      scalar_optim: "adamw"
      kwargs:
        lr: 3.0e-6
        weight_decay: 0.01
        betas: [0.9, 0.999]
      scheduler:
        phases:
          - name: "torch.optim.lr_scheduler.ConstantLR"
            kwargs:
              factor: 1.0
              total_iters: 10000000000
        milestones: []

    loss:
      loss_fn: clipped_pg
      ratio_clip_min: 0.2
      ratio_clip_max: 0.2
      use_importance_sampling_correction: true
      disable_ppo_ratio: false

    resources:
      gpus_per_node: 1
      num_nodes: 1

  inference:
    tp_size: 1
    gpu_memory_utilization: 0.6
    dtype: "bfloat16"
    server_timeout: 6000
    sampling_args:
      temperature: 1.0
      top_p: 1.0
    resources:
      gpus_per_node: 1
      num_nodes: 1

env:
  env_name: "vf_reverse_text"
  env_kwargs: {}
  max_prompt_length_ratio: 1.0
  shuffle: false

logging:
  log_dir: "logs"
  wandb:
    project: "nemo-grpo-dev"
    name: "grpo-vf-reverser-0.6B"
  gpu_monitoring:
    collection_interval: 10
    flush_interval: 10

checkpointing:
  enabled: true
  checkpoint_dir: "results/grpo_vf_reverser_600M"
  save_period: 10
  keep_top_k: 3
