sft:
  max_steps: 9999999  # Train until data exhausted or this limit
  val_period: 100
  val_batches: 8
  val_at_start: true
  seed: 42

policy:
  model_name: "arcee-train/hf-converted-afmoe-nano-ctxext-v4"
  max_total_sequence_length: 4096

  training:
    global_num_bins: 128
    micro_batch_size: 1
    dtype: "bfloat16"
    activation_checkpointing: true
    max_grad_norm: 1.0

    parallelism:
      tp_size: 1
      ep_size: 1
      dp_replicate: 2

    optimizer:
      name: "torchao.optim.AdamW8bit"
      kwargs:
        lr: 3.0e-6
        weight_decay: 0.01
        betas: [0.9, 0.999]
        eps: 1.0e-8
      scheduler:
        phases:
          - name: "torch.optim.lr_scheduler.LinearLR"
            kwargs:
              start_factor: 1.0e-6
              end_factor: 1.0
              total_iters: 100
          - name: "torch.optim.lr_scheduler.LinearLR"
            kwargs:
              start_factor: 1.0
              end_factor: 0.7
              total_iters: 700
          - name: "torch.optim.lr_scheduler.LinearLR"
            kwargs:
              start_factor: 0.7
              end_factor: 1.0e-6
              total_iters: 633
        milestones: [100, 800]

    loss:
      loss_fn: cut_cross_entropy

    resources:
      gpus_per_node: 2
      num_nodes: 1

data:
  train_dataset: "arcee-train/AFMoE-SFT-v0.1.9-64k-RLKit-Fixed"
  val_dataset: null
  dataset_type: "native"
  from_disk: false
  shuffle: false

logging:
  log_dir: "logs"
  wandb:
    project: "AFMoE-6b-SFT-Debug"
    name: "afmoe-nano-sft"
  gpu_monitoring:
    collection_interval: 10
    flush_interval: 10

checkpointing:
  enabled: true
  checkpoint_dir: "results/sft"
  save_period: 200
  keep_top_k: 3
