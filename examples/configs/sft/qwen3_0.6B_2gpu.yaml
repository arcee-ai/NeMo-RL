# SFT Training Configuration
sft:
  max_steps: 1000  # 0 = train until data exhausted
  val_period: 100  # Run validation every N steps (0 = no validation)
  val_batches: 10  # Batches per validation (0 = full validation set)
  val_at_start: false
  seed: 42

policy:
  model_name: "Qwen/Qwen2.5-0.5B"
  max_total_sequence_length: 2048

  training:
    global_batch_size: 32
    micro_batch_size: 4
    dtype: "bfloat16"
    activation_checkpointing: false
    max_grad_norm: 1.0

    parallelism:
      tp_size: 1
      ep_size: 1
      dp_replicate: 1

    optimizer:
      name: "torch.optim.AdamW"
      kwargs:
        lr: 1.0e-5
        weight_decay: 0.01
        betas: [0.9, 0.999]
        eps: 1.0e-8
        foreach: false
        fused: false
      scheduler:
        phases:
          - name: "torch.optim.lr_scheduler.CosineAnnealingLR"
            kwargs:
              T_max: 1000
              eta_min: 1.0e-6
        milestones: []

    loss:
      loss_fn: nll

    resources:
      gpus_per_node: 2
      num_nodes: 1

data:
  train_dataset: "HuggingFaceH4/ultrachat_200k"  # Or local path
  val_dataset: null  # Optional validation dataset
  dataset_type: "openai"  # native, axolotl, openai, openai_prompt_completion, sharegpt
  from_disk: false
  shuffle: true

logging:
  log_dir: "logs/sft"
  wandb:
    project: "nemo-sft"
    name: "sft-qwen-0.5B"
  gpu_monitoring:
    collection_interval: 10
    flush_interval: 10

checkpointing:
  enabled: true
  checkpoint_dir: "results/sft_qwen_0.5B"
  save_period: 100
  keep_top_k: 3
