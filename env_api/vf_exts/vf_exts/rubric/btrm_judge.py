import logging
import re
import verifiers as vf
from verifiers.types import Messages, State, Info, RolloutScores

from typing import List

from openai import OpenAI

from .grouped_rubric import GroupedRubric

DEFAULT_JUDGE_PROMPT = """
Below are two responses generated by a large language model. Please judge which response is better, according to the rubric provided.

Your final answer should be in the form of an integer between 1 and 7, inclusive.
A response of 1 means that Response A is much better than Response B, 7 means that Response B is much better than Response A, and 4 means that the two responses are equally good.

Return your answer in XML tags, like this:

<answer>4</answer>

Reply with no other text.

<rubric>
{}
</rubric>

<response_a>
{}
</response_a>

<response_b>
{}
</response_b>
"""

class PairwiseJudgeRubric(GroupedRubric):
    def __init__(
        self,
        judge_client: OpenAI | None = None,
        judge_model: str = "gpt-4.1-nano",
        judge_prompt: str = DEFAULT_JUDGE_PROMPT,
        rubric_prompt: str = "Pick whichever response is better.",
        judge_sampling_args: dict = {},
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.judge_client = judge_client if judge_client is not None else OpenAI()
        self.judge_model = judge_model
        self.judge_prompt = judge_prompt
        self.rubric_prompt = rubric_prompt
        self.judge_sampling_args = judge_sampling_args

    async def score_rollouts_grouped(
        self,
        prompts: List[Messages],
        completions: List[Messages],
        answer: str,
        states: List[State],
        task: str,
        info: Info,
        **kwargs,
    ) -> RolloutScores:
        assert len(completions) % 2 == 0, "Number of completions must be even for pairwise comparison"
        
        was_judge_malformed = []
        
        rewards = []
        for i in range(0, len(completions), 2):
            a = completions[i][-1]["content"]
            b = completions[i+1][-1]["content"]
            prompt = self.judge_prompt.format(self.rubric_prompt, a, b)
            
            response = self.judge_client.chat.completions.create(
                model=self.judge_model,
                messages=[{"role": "user", "content": prompt}],
                **self.judge_sampling_args
            )
            
            was_malformed = False
            
            match = re.search(r"<answer>(.*?)</answer>", response.choices[0].message.content)
            if match is None:
                logging.error(f"No answer found in response: {response.choices[0].message.content}")
                was_malformed = True
                opinion = 4
            else:
                try:
                    opinion = int(match.group(1))
                except ValueError:
                    logging.error(f"Model gave invalid opinion: {match.group(1)}")
                    was_malformed = True
                    opinion = 4
            
            if opinion < 1 or opinion > 7:
                logging.error(f"Model gave invalid opinion: {opinion}")
                was_malformed = True
                opinion = 4
            
            print(f"opinion: {opinion} (was_malformed: {was_malformed})")
            print(f"prompt: {prompt}")
            print(f"response: {response.choices[0].message.content}")
            
            # We have to do this twice so the resulting metric is the same size as the group.
            was_judge_malformed.append(was_malformed)
            was_judge_malformed.append(was_malformed)
            
            rewards.append(4 - opinion) # A is highest when opinion is 1
            rewards.append(opinion - 4) # B is highest when opinion is 7
            
        return RolloutScores(reward=rewards, metrics={"was_judge_malformed": was_judge_malformed})
            